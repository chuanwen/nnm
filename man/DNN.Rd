% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/dnn.R
\name{DNN}
\alias{DNN}
\alias{Strip.DNN}
\alias{Forward.DNN}
\alias{Backward.DNN}
\alias{UpdateParameters.DNN}
\alias{learning.DNN}
\alias{predict.DNN}
\alias{predict.DNNClassifier}
\title{A S3 class to represent a deep neuro net.}
\usage{
DNN(layer, loss, type = c("regression", "classification", "other"))

\method{Strip}{DNN}(object)

\method{Forward}{DNN}(object, x, ...)

\method{Backward}{DNN}(object, y, weights)

\method{UpdateParameters}{DNN}(object, learningRate)

\method{learning}{DNN}(object, x, y, weights, learningRate, ...)

\method{predict}{DNN}(object, newdata, ...)

\method{predict}{DNNClassifier}(object, newdata, type = c("response", "label"))
}
\arguments{
\item{layer}{a Layer object, which contains many layers. Usually you construct}

\item{loss}{a Loss object}

\item{type}{type of the network, for classification, it's predict Generic}

\item{object}{an DNN object}

\item{x}{numeric matrix of dimension p x nobs, i.e. each col is a predictor vector.}

\item{y}{a numeric or factor vector, or a numeric matrix of dimension
q x nobs, i.e. each col is a response vector. If y is a vector, it will be
converted to a matrix (nrow=1) in Backward.DNN.}
}
\description{
A S3 class to represent a deep neuro net.
}
\section{Methods (by generic)}{
\itemize{
\item \code{Strip}: strip the net to only keep what is needed for Forward method.

\item \code{Forward}: implement generic Forward function

\item \code{Backward}: implement  DNNBackward function.

\item \code{UpdateParameters}: implement generic UpdateParameters function.

\item \code{learning}: implement generic learning function.

\item \code{predict}: prediction of Network

\item \code{predict}: prediction of class probability or labels when last layer is softmax
}}

\examples{
inputDim = 8
nObs = 100
x = array(rnorm(prod(inputDim)*nObs), dim=c(inputDim, nObs))
y = 0.1 * rnorm(nObs) + x[1, ]
layer1 = Parallel(Dense(4, 2, Activation.Identity), Dense(4, 2))
layer2 = Dense(4, 1, Activation.Identity)
layer = Sequential(layer1, layer2)
dnn = DNN(layer, MSELoss, type="regression")
for (i in 1:20) {
   dnn = learning(dnn, x, y)
}
dnn
plot(y, predict(dnn, x))

}
