% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/nnm.R
\docType{package}
\name{nnm}
\alias{nnm}
\title{Neuron Network Model}
\usage{
nnm(
  x,
  y,
  layerSpecs,
  embeddingCols = NULL,
  numEmbeddingDims = 0,
  weights,
  family,
  ...
)
}
\arguments{
\item{x}{matrix or data.frame with numeric, factor or character cols.
Each col is a variable or predictor, each row is an observation.}

\item{y}{response variable (vector or matrix). Quantitative for
"gaussian" family, or "poisson" family (non-negative counts).
For "multinomial" family, should be either a factor/character vector, or
a matrix of N-Classes cols.}

\item{layerSpecs}{either a list of layer specs, or a (composite) layer
objectect (e.g. a Sequential or DAG layer). See \code{\link{Sequential}}
for how to specify a sequential of layers,}

\item{embeddingCols}{specify factor cols to be embedded. Factor cols
that are not embeded would be encoded using default contrast matrix.}

\item{numEmbeddingDims}{specify the num of embedding dims, if length(numEmbeddingDims)
is only a fraction of length(embeddingCols), then
numEmbeddingDims would be repeated to the same length of embeddingCols.}

\item{weights}{weight of the obs, default 1 for each and every observation.}

\item{family}{Response type, should be one of c("gaussian", "poisson",
"multinomial"). If not specified, will be inferred from class(y).}

\item{nEpoch}{number of times to train over the whole data, default 5.}

\item{batchSize}{number of observations per train batch, default 100.}

\item{learningRate}{a float between 0 and 1, default 0.25.}
}
\description{
Fit a neuron network model.
}
\examples{
x <- iris[, 1:4]
y <- iris[, 5]
mod <- nnm(x, y, list(Dense(4, 3, Activation.Identity), Softmax))
mod

# example usage when x has factor cols
y <- iris[, 1]
x <- iris[, 2:5]
layerSpecs <- list(Dense(3+nlevels(x$Species), 2), Dense(2, 1, Activation.Identity))
mod <- nnm(x, y, layerSpecs)
mod

# example usage of embedding
y <- iris[, 1]
x <- iris[, 2:5]
embeddingCols <- "Species"
numEmbeddingDims <- 2
layerSpecs <- list(Dense(3+numEmbeddingDims, 2), Dense(2, 1, Activation.Identity))
mod <- nnm(x, y, layerSpecs, embeddingCols, numEmbeddingDims)
mod

# example model for MNIST data
mnist <- LoadMnist()
train <- mnist$train
test <- mnist$test
layerSpec <- list(
   Dense(784, 128),
   Dropout(128, keepProb=0.8),
   Dense(128, 10, Activation.Identity),
   Softmax)
layerSpec
mod <- nnm(train$x, train$y, layerSpec, verbose=1, nEpoch=2)
# accuracy on train set
mean(mod$y == mod$fitted)
# accuracy on test set
mean(test$y == predict(mod, test$x, type="label"))

}
